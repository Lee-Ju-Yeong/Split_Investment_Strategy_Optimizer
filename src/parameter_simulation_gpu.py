"""
parameter_simulation_gpu.py

GPU-accelerated parameter simulation for the MagicSplitStrategy.
This script orchestrates the backtesting of thousands of parameter combinations
by leveraging CuPy and CuDF for massive parallelization on the GPU.
"""

import time
import cudf
import cupy as cp
import pandas as pd
from sqlalchemy import create_engine
from datetime import timedelta, datetime  # timedelta ì„í¬íŠ¸ ì¶”ê°€
import os # os ì¶”ê°€
import urllib.parse
# --- í•„ìš”í•œ ëª¨ë“ˆ ì¶”ê°€ ì„í¬íŠ¸ ---
from src.config_loader import load_config
from src.backtest_strategy_gpu import run_magic_split_strategy_on_gpu
from src.performance_analyzer import PerformanceAnalyzer

# -----------------------------------------------------------------------------
# 1. Configuration and Parameter Setup
# -----------------------------------------------------------------------------

import urllib.parse

# --- ì„¤ì • íŒŒì¼ ë¡œë“œ (YAML ë¡œë”ë¡œ í†µì¼) ---
config = load_config()
execution_params = config["execution_params"]  # configì—ì„œ íŒŒë¼ë¯¸í„° ë¡œë“œ
db_config = config["database"]
backtest_settings = config["backtest_settings"]

# URL ì¸ì½”ë”©ì„ í¬í•¨í•˜ì—¬ DB ì—°ê²° ë¬¸ìì—´ ìƒì„±
db_pass_encoded = urllib.parse.quote_plus(db_config["password"])
db_connection_str = (
    f"mysql+pymysql://{db_config['user']}:{db_pass_encoded}"
    f"@{db_config['host']}/{db_config['database']}"
)

# Define the parameter space to be tested
max_stocks_options = cp.array([24], dtype=cp.int32)
order_investment_ratio_options = cp.array([0.02], dtype=cp.float32)
additional_buy_drop_rate_options = cp.array([0.04], dtype=cp.float32)
sell_profit_rate_options = cp.array([0.04], dtype=cp.float32)
additional_buy_priority_options = cp.array(
    [0,1], dtype=cp.int32
)  # 0: lowest_order, 1: highest_drop

# Create all combinations using CuPy's broadcasting capabilities
grid = cp.meshgrid(
    max_stocks_options,
    order_investment_ratio_options,
    additional_buy_drop_rate_options,
    sell_profit_rate_options,
    additional_buy_priority_options,
)
# Flatten the grid to get a list of all combinations
param_combinations = cp.vstack([item.flatten() for item in grid]).T
num_combinations = param_combinations.shape[0]

print(f"âœ… Total parameter combinations generated for GPU: {num_combinations}")


# -----------------------------------------------------------------------------
# 2. GPU Data Pre-loader
# -----------------------------------------------------------------------------


def preload_all_data_to_gpu(engine, start_date, end_date):
    """
    Loads all necessary stock data for the entire backtest period into a
    single cuDF DataFrame, minimizing I/O during simulation.

    Returns:
        cudf.DataFrame: A DataFrame containing all OHLCV and indicator data,
                        indexed by (ticker, date).
    """
    print("â³ Loading all stock data into GPU memory...")
    start_time = time.time()

    query = f"""
    SELECT 
        stock_code AS ticker, 
        date, 
        open_price, 
        high_price, 
        low_price, 
        close_price, 
        volume,
        atr_14_ratio
    FROM DailyStockPrice
    JOIN CalculatedIndicators USING (stock_code, date)
    WHERE date BETWEEN '{start_date}' AND '{end_date}'
    """

    # Use cuDF to read directly from the database into a GPU DataFrame
    # Note: This requires a compatible database connector and setup.
    # For now, we load via pandas as an intermediate step.

    # Create a SQLAlchemy engine
    sql_engine = create_engine(engine)

    # Load data using pandas first
    df_pd = pd.read_sql(query, sql_engine, parse_dates=["date"])

    # Move the DataFrame to the GPU
    gdf = cudf.from_pandas(df_pd)

    # Set a multi-index for efficient lookups
    gdf = gdf.set_index(["ticker", "date"])

    end_time = time.time()
    print(
        f"âœ… Data loaded to GPU. Shape: {gdf.shape}. Time: {end_time - start_time:.2f}s"
    )

    return gdf


def preload_weekly_filtered_stocks_to_gpu(engine, start_date, end_date):
    """
    Loads all weekly filtered stock codes for the backtest period into a
    cuDF DataFrame.
    """
    print("â³ Loading weekly filtered stocks data to GPU memory...")
    start_time = time.time()
    
    # â˜…â˜…â˜… í•µì‹¬ ìˆ˜ì • â˜…â˜…â˜…
    # ë°±í…ŒìŠ¤íŒ… ì‹œì‘ì¼ë³´ë‹¤ ë„‰ë„‰í•˜ê²Œ 2ì£¼ ì „ ë°ì´í„°ë¶€í„° ë¡œë“œí•˜ì—¬,
    # ì—°ì´ˆì— ì´ì „ ë…„ë„ ë°ì´í„°ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ í•¨
    extended_start_date = pd.to_datetime(start_date) - timedelta(days=14)
    extended_start_date_str = extended_start_date.strftime('%Y-%m-%d')
    
    # WeeklyFilteredStocks í…Œì´ë¸”ì—ì„œ í•´ë‹¹ ê¸°ê°„ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
    query =  f"""
    SELECT `filter_date` as date, `stock_code` as ticker
    FROM WeeklyFilteredStocks
    WHERE `filter_date` BETWEEN '{extended_start_date_str}' AND '{end_date}'
    """
    sql_engine = create_engine(engine)
    df_pd = pd.read_sql(query, sql_engine, parse_dates=['date'])
    
    # cuDFë¡œ ë³€í™˜
    gdf = cudf.from_pandas(df_pd)
    gdf = gdf.set_index('date')
    
    end_time = time.time()
    print(f"âœ… Weekly filtered stocks loaded to GPU. Shape: {gdf.shape}. Time: {end_time - start_time:.2f}s")
    return gdf


# -----------------------------------------------------------------------------
# 3. GPU Backtesting Kernel (to be implemented)
# -----------------------------------------------------------------------------


def run_gpu_optimization(
    params_gpu,
    data_gpu,
    weekly_filtered_gpu,
    all_tickers,
    trading_date_indices_gpu,
    trading_dates_pd,
    initial_cash_value,
    execution_params,
    debug_mode: bool = False,
):
    """
    GPU-accelerated backtestingì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
    """
    print("ğŸš€ Starting GPU backtesting kernel...")

    # GPU ë°±í…ŒìŠ¤íŒ… í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ
    daily_portfolio_values = run_magic_split_strategy_on_gpu(
        initial_cash=initial_cash_value,  # <<< ìˆ˜ì •ëœ ì¸ì ì´ë¦„ ë° ì™¸ë¶€ì—ì„œ ë°›ì€ ê°’ ì‚¬ìš©
        param_combinations=params_gpu,
        all_data_gpu=data_gpu,
        weekly_filtered_gpu=weekly_filtered_gpu,
        trading_date_indices=trading_date_indices_gpu,
        trading_dates_pd_cpu=trading_dates_pd,
        all_tickers=all_tickers,
        execution_params=execution_params,  # â˜…â˜…â˜… ì¶”ê°€ëœ ì¸ì ì „ë‹¬
        max_splits_limit=20,
        debug_mode=debug_mode,
    )

    print("ğŸ‰ GPU backtesting kernel finished.")

    return daily_portfolio_values

# parameter_simulation_gpu.py íŒŒì¼ì— ìˆëŠ” ì´ í•¨ìˆ˜ë¥¼ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•˜ì„¸ìš”.

def analyze_and_save_results(param_combinations_gpu, daily_values_gpu, trading_dates_pd, initial_cash):
    """
    Analyzes the results from the GPU backtest, calculates detailed metrics,
    prints the top performers, and saves the full results to a CSV file.

    Args:
        param_combinations_gpu (cp.ndarray): The parameter combinations tested.
        daily_values_gpu (cp.ndarray): The daily portfolio values for each combination.
        trading_dates_pd (pd.DatetimeIndex): The trading dates for the backtest period.
        initial_cash (float): The initial capital for the backtest.
    """
    print("\n--- ğŸ”¬ Analyzing detailed performance metrics ---")
    start_time = time.time()
    
    # 1. GPUì—ì„œ CPUë¡œ ë°ì´í„° ì´ë™
    param_combinations_cpu = param_combinations_gpu.get()
    daily_values_cpu = daily_values_gpu.get()
    
    results_list = []
    num_combinations = daily_values_cpu.shape[0]

    # 2. ê° íŒŒë¼ë¯¸í„° ì¡°í•©ì— ëŒ€í•´ ìƒì„¸ ì§€í‘œ ê³„ì‚°
    for i in range(num_combinations):
        # ì¼ë³„ ê°€ì¹˜ ë°ì´í„°ë¥¼ Pandas Seriesë¡œ ë³€í™˜
        daily_series = pd.Series(daily_values_cpu[i], index=trading_dates_pd)
        
         ### ### [í•µì‹¬ ìˆ˜ì •] Seriesë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ### ###
        # PerformanceAnalyzerê°€ ìš”êµ¬í•˜ëŠ” 'total_value' ì»¬ëŸ¼ì„ ê°€ì§„ DataFrame ìƒì„±
        history_df_mock = pd.DataFrame(daily_series, columns=['total_value'])
        
        # PerformanceAnalyzerë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€í‘œ ê³„ì‚°
        analyzer = PerformanceAnalyzer(history_df_mock)
        metrics = analyzer.get_metrics(formatted=False) # ì›ë³¸ ìˆ«ì ë°ì´í„°ë¡œ ë°›ê¸°
        results_list.append(metrics)

    # 3. íŒŒë¼ë¯¸í„°ì™€ ì„±ê³¼ ì§€í‘œë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©
    param_names = ['max_stocks', 'order_investment_ratio', 'additional_buy_drop_rate', 'sell_profit_rate', 'additional_buy_priority']
    params_df = pd.DataFrame(param_combinations_cpu, columns=param_names)
    metrics_df = pd.DataFrame(results_list)
    
    full_results_df = pd.concat([params_df, metrics_df], axis=1)

    # 4. Calmar Ratio ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ì¶œë ¥
    # Calmar Ratioê°€ ë¬´í•œëŒ€(inf)ë‚˜ NaNì¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì •ë ¬ ì „ì— ì²˜ë¦¬
    full_results_df.replace([cp.inf, -cp.inf], cp.nan, inplace=True)
    sorted_df = full_results_df.sort_values(by='calmar_ratio', ascending=False).dropna(subset=['calmar_ratio'])

    print("\nğŸ† Top 10 Performing Parameter Combinations (by Calmar Ratio):")
    
    # ì¶œë ¥í•  ì»¬ëŸ¼ ì„ íƒ ë° í¬ë§·íŒ…
    display_columns = [
        'calmar_ratio', 'cagr', 'mdd', 'sharpe_ratio', 'sortino_ratio', 'annualized_volatility',
        'max_stocks', 'order_investment_ratio', 'additional_buy_drop_rate', 'sell_profit_rate'
    ]
    # 'additional_buy_priority'ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¶”ê°€
    if 'additional_buy_priority' in sorted_df.columns:
        display_columns.append('additional_buy_priority')
        
    display_df = sorted_df.head(10).get(display_columns, pd.DataFrame())

    if not display_df.empty:
        # ìˆ«ì í¬ë§·íŒ…
        display_df['calmar_ratio'] = display_df['calmar_ratio'].map('{:.2f}'.format)
        display_df['cagr'] = display_df['cagr'].map('{:.2%}'.format)
        display_df['mdd'] = display_df['mdd'].map('{:.2%}'.format)
        display_df['sharpe_ratio'] = display_df['sharpe_ratio'].map('{:.2f}'.format)
        display_df['sortino_ratio'] = display_df['sortino_ratio'].map('{:.2f}'.format)
        display_df['annualized_volatility'] = display_df['annualized_volatility'].map('{:.2%}'.format)
        
        print(display_df.to_string(index=False))
    else:
        print("ê²°ê³¼ë¥¼ í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 5. ì „ì²´ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = 'results'
    os.makedirs(output_dir, exist_ok=True)
    filepath = os.path.join(output_dir, f'gpu_simulation_results_{timestamp}.csv')
    
    sorted_df.to_csv(filepath, index=False, float_format='%.4f')
    
    end_time = time.time()
    print(f"\nâœ… Full analysis saved to: {filepath}")
    print(f"â±ï¸  Analysis and saving took: {end_time - start_time:.2f} seconds.")

# -----------------------------------------------------------------------------
# 4. Main Execution Block
# -----------------------------------------------------------------------------

if __name__ == "__main__":
    backtest_start_date = backtest_settings["start_date"]
    backtest_end_date = backtest_settings["end_date"]
    initial_cash = backtest_settings[
        "initial_cash"
    ]  # <<< configì—ì„œ ì´ˆê¸° ìë³¸ ê°€ì ¸ì˜¤ê¸°

    print(f"ğŸ“… í…ŒìŠ¤íŠ¸ ê¸°ê°„: {backtest_start_date} ~ {backtest_end_date}")

    # 1. Pre-load all data to GPU
    all_data_gpu = preload_all_data_to_gpu(
        db_connection_str, backtest_start_date, backtest_end_date
    )
    # ğŸ’¡ ìƒˆë¡œìš´ ë°ì´í„° ë¡œë” í˜¸ì¶œ: ì£¼ê°„ í•„í„°ë§ëœ ì¢…ëª© ë°ì´í„° ë¡œë“œ
    weekly_filtered_gpu = preload_weekly_filtered_stocks_to_gpu(
        db_connection_str, backtest_start_date, backtest_end_date
    )

    # --- ğŸ’¡ ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ ğŸ’¡ ---

    # 2. Generate trading dates and convert them to integer indices for GPU
    # Pandasì˜ bdate_rangeë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ê±°ë˜ì¼ë§Œ ê°€ì ¸ì˜´
    trading_dates_pd = pd.bdate_range(start=backtest_start_date, end=backtest_end_date)

    # GPU ì»¤ë„ì—ì„œëŠ” 0, 1, 2... ì™€ ê°™ì€ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë‚ ì§œë¥¼ ìˆœíšŒ
    trading_date_indices_gpu = cp.arange(len(trading_dates_pd), dtype=cp.int32)

    # 3. Filter the main GPU DataFrame to include only actual trading dates
    #    This ensures the GPU data aligns with our trading date indices.
    #    cuDFëŠ” datetime ê°ì²´ë¥¼ ì¸ë±ìŠ¤ë¡œ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
    all_data_gpu = all_data_gpu[
        all_data_gpu.index.get_level_values("date").isin(trading_dates_pd)
    ]

    # --- ğŸ’¡ ìˆ˜ì •ëœ ë¶€ë¶„ ë ğŸ’¡ ---

    # 4. Get all tickers from the loaded data
    #    (all_data_gpuê°€ í•„í„°ë§ë˜ì—ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œ í‹°ì»¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ì •í™•í•¨)
    all_tickers = (
        all_data_gpu.index.get_level_values("ticker").unique().to_pandas().tolist()
    )
    print(f"ğŸ“Š ë¡œë“œëœ ì¢…ëª© ìˆ˜: {len(all_tickers)}")
    print(f"ğŸ“Š ì‹¤ì œ ê±°ë˜ì¼ ìˆ˜: {len(trading_date_indices_gpu)}")

    # 5. Run the backtesting kernel
    print(f"\nğŸš€ {num_combinations}ê°œ íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ GPU ë°±í…ŒìŠ¤íŒ… ì‹œì‘...")
    start_time = time.time()

    # í•¨ìˆ˜ ì´ë¦„ ë³€ê²½ ë° initial_cash ì „ë‹¬
    daily_values_result = run_gpu_optimization(
        param_combinations,
        all_data_gpu,
        weekly_filtered_gpu,
        all_tickers,
        trading_date_indices_gpu,
        trading_dates_pd,
        initial_cash,
        execution_params,
        debug_mode=False, # ëŒ€ê·œëª¨ ì‹¤í–‰ ì‹œì—ëŠ” ë””ë²„ê·¸ ëª¨ë“œ ë¹„í™œì„±í™”
    )

    end_time = time.time()
    elapsed_time = end_time - start_time

    # 5. Results summary
    print(f"\n--- ğŸ‰ GPU ë°±í…ŒìŠ¤íŒ… ì™„ë£Œ ---")
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    print(f"ğŸ“ˆ ì¡°í•© ë‹¹ í‰ê·  ì‹œê°„: {elapsed_time/num_combinations*1000:.2f}ms")
    print(
        f"ğŸ”¥ CPU ëŒ€ë¹„ ì˜ˆìƒ ê°€ì†ë„: {8 * elapsed_time / (num_combinations * 0.1):.1f}x"
    )


    # ### ì´ìŠˆ #3 êµ¬í˜„: ìƒˆë¡œìš´ ë¶„ì„ ë° ì €ì¥ í•¨ìˆ˜ í˜¸ì¶œ ###
    analyze_and_save_results(param_combinations, daily_values_result, trading_dates_pd, initial_cash)
    
    print(f"\nâœ… GPU íŒŒë¼ë¯¸í„° ìµœì í™” ë° ë¶„ì„ ì™„ë£Œ!")